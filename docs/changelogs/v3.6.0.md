## Change Log

### Feature

* Implement `Fira` optimizer. (#376)
    * [Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?](https://arxiv.org/abs/2410.01623) 
* Implement `RACS` and `Alice` optimizers. (#376)
    * [Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension](https://arxiv.org/abs/2502.07752)
* Implement `VSGD` optimizer. (#377, #378)
    * [Variational Stochastic Gradient Descent for Deep Neural Networks](https://openreview.net/forum?id=xu4ATNjcdy) 
* Support complex parameters. (#370, #380)
* Support `maximize` parameter. (#370, #380)

### Update

* Support 2D< Tensor for `RACS` and `Alice` optimizers. (#380)
* Remove the auxiliary variants from the default parameters of the optimizers and change the name of the state and parameter. (#380)
    * `use_gc`, `adanorm`, `cautious`, `stable_adamw`, and `adam_debias` will be affected.
    * You can still use these variants by passing the parameters to `**kwargs`.
    * Notably, in case of `adanorm` variant, you need to pass `adanorm` (and `adanorm_r` for `r` option) parameter(s) to use this variant, and the name of the state will be changed from `exp_avg_norm` to `exp_avg_adanorm`.
* Remove `reset()` method from the `BaseOptimizer` class and refactor to `init_group()` method. (#380)

### Fix

* Fix shape mismatch issues in the Galore projection for `reverse_std`, `right` and `full` projection types. (#376)
