## Change Log

### Feature

* Implement `WSD` LR Scheduler. (#247, #248)
  * [Warmup-Stable-Decay LR Scheduler](https://arxiv.org/abs/2404.06395)
* Add more Pytorch built-in lr schedulers. (#248)
* Implement `Kate` optimizer. (#249, #251)
  * [Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad](https://arxiv.org/abs/2403.02648) 

### Refactor

* Refactor `Chebyschev` lr scheduler modules. (#248)
  * Rename `get_chebyshev_lr` to `get_chebyshev_lr_lambda`.
  * Rename `get_chebyshev_schedule` to `get_chebyshev_perm_steps`.
  * Call `get_chebyshev_schedule` function to get `LamdbaLR` scheduler object.
* Refactor with `ScheduleType`. (#248)
